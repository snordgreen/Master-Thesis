% !TEX encoding = UTF-8 Unicode
%!TEX root = thesis.tex
% !TEX spellcheck = en-US
%%=========================================
\chapter[Discussion and Conclusions]{Discussion and Conclusions}
%%This is the last chapter

A thorough discussion is warranted in order to properly contextualize and scrutinize  the various choices, experiences and results that have been utilized or obtained throughout this study. Most prominently, this pertains to the set of assumptions and choices that were made in absence of an adequate preexisting method for LCS identification in three dimensions. Furthermore, the results presented in chapter \ref{ch:Results} warrant some consideration. Finally, exploring the successes and shortcomings of this exercise yields suggestions for further work in the field.

%%=========================================
\section{Computing Cauchy-Green eigenvalue and eigenvector fields}\label{sec:discussion_eigen}

As outlined in section \ref{sec:tracer_advection}, tracer advection and calculation of the flow map and flow map Jacobian was performed by use of the variational equations \eqref{eq:var_eq_1} and \eqref{eq:var_eq_2}. This approach, suggested by \cite{Oettinger}, was chosen due to its superior accuracy. While some complexity is added by solving a set of twelve equations as opposed to three, we avoid having to compute auxiliary grids \citep{Haller12} and introducing the accompanying error from finite differencing. It should however be noted that this method requires bounded first order velocity field partial derivatives with respect to the spatial coordinates (see equation \eqref{eq:var_eq_2_cartesian}). This requires us to use
differentiable analytical test cases and higher order interpolation methods for gridded data models. Cases in which this is impossible or impractical should therefore be handled by use of auxiliary grids and finite differencing, as outlined by \cite{Haller12}.

Also pertaining to velocity field interpolation is the use of cubic quadrivariate interpolation. Requiring us to keep the entire velocity field data set in working memory while the interpolation is being used, this approach may be prohibitively expensive for some applications. Consider for example a data set of $100$ time steps, over a $500\times500\times100$ grid of three component double precision velocity vectors. Using quadrivariate interpolation for this data set requires us to keep in excess of 60 GB in memory while advecting tracer particles, which is impractical. This issue may be resolved by dividing the time interval into several segments, each computed with separate interpolations that are passed from memory whenever we leave the corresponding time segment. Note that the number of time steps included in each segment must allow the desired order of time dimension interpolation. In the case of cubic interpolation, this corresponds to four time steps or more. 

Alternatively, we may drop the quadrivariate interpolation in favor of trivariate interpolation in the three spatial dimensions. Using a Runge-Kutta method without intermediate step slope approximations, such as the trapezoidal rule \citep{SolvingODEs}, allows us to solve the system of \eqref{eq:var_eq_1} and \eqref{eq:var_eq_2} without time interpolation. It should however be noted that this approach prohibits us from detaching our solution steps from the time step of the data set. Moreover, using a lower order integrator such as the second order trapezoidal rule carries significant disadvantages, as such methods are inferior to higher order adaptive step methods in terms of efficiency. 

%The use of singular value decomposition, described in sections \ref{sec:SVD} and  \ref{sec:Cauchy-Green_eigen} was chosen as to avoid the loss of information associate with computing the Cauchy-Green strain tensor \citep{Watkins05}. ANYTHING ELSE TO DISCUSS HERE? -> OTHERWISE SKIP

It should be noted that tracer initial position grid density could be critical in terms of identifying some LCSs. Specifically, the spherical test case LCS described in section \ref{sec:LCS_test_cases} was indiscernible when using a lower tracer initial position grid density. This is reasonable, as no manifold initial positions were placed sufficiently close to the sphere of heightened repulsion with these configurations. It is hard to tell \textit{a priori} what tracer density is sufficient for any given application. We therefore recommend to run the highest tracer initial position grid density that is practical on the available hardware setup.

%[[Use of SVD, use of variational equations, use of quadvariate interpolation instead of trivariate and RK2, choice of interpolation order -> no less than cubic due to second derivatives -> could do higher but more sketchy for eigenvectors, cubic better than linear in terms of handling peaks, RKDP87 found to be more efficient, tracer grid density critical -> how sharp LCSs can we resolve]]

%%=========================================
\section{Adapting the method of geodesic levelsets}\label{sec:discussion_GLS}

Managing the added degree of freedom gained when defining manifolds according to equation \eqref{eq:normal_field}, compared to manifolds defined by ODEs of the form $\vec{x}'=\vec{f}(\vec{x})$, the method of geodesic levelsets described by \cite{GeodesicLevelSets} was modified by forcing radial trajectories. As previously discussed in section \ref{sec:force_radially_outward}, this approach was preferred over that of guided trajectories due to superior speed, clarity, and accuracy. The approach of radially forced trajectories with inherited target half-planes $\mathcal{F}_r$ also permits a departure from the structuring of points into topological circles. Although useful in terms of managing mesh grid density, as well as guiding our triangulation algorithm, this hierarchical structure carries major disadvantages.

As highlighted by the fjord current LCS case (see section \ref{sec:fjord}), the organization of points into levelsets, approximating topological circles, requires us to use periodic eigenvector field boundary conditions in order to consistently determine the boundary behavior of the LCS system. In section \ref{sec:boundary_treatment}, we noted that this treatment may introduce false positive LCS fragments if mesh point strands return to $U$ after initially having moved beyond the boundaries of the domain of interest.

%While the issue of boundary description can be managed by use of an extended initial position tracer grid domain, this is computationally demanding and in some cases unworkable. That is, by defining eigenvalue and eigenvector fields extending outside $U$, boundary behavior may be more adequately explored. However, an alternative, and much less costly, solution involves partly departing from the convention of adding manifold points only as complete levelsets representing topological circles.

This issue can be managed by use of an extended initial position tracer grid domain. That is, by defining eigenvalue and eigenvector fields extending outside
U, boundary behavior may be more adequately explored. However, this approach is computationally demanding and in some cases unworkable. An alternative, and much less costly, solution involves partly departing from the convention of adding
manifold points only as complete levelsets representing topological circles. This may be done by organizing points into strands of descendants, each associated with a certain tangent line on the initial circular levelset curve $C_1$. This allows us to continue adding points even after parts of a levelset has left the region of interest $U$. Moreover, this partly bypasses the problem of point search trajectories potentially never reaching an acceptable new point. The strand in question would stop, but all other active strands could continue irrespective of this local error.

Organizing points into descendant strands instead of levelsets forming topological circles raises the question of how to manage grid density. This is because the topological circles $\{C_i\}$ are useful for inserting ghost ancestor points wherever inserting an additional point is necessary. We could handle this by simply computing a new strand using an intervening target half-plane $\mathcal{F}_r$. Discarding all unnecessary points, we would then be left with the required point. Note that in order to maintain a similar point structure conducive to proper mesh triangulation, the constant inter-levelset --- or in this case inter-strand point --- step length should be kept equal. That is, strand step $n$ in strand $i$, should be of the same length as strand step length $n$ in strand $j\neq i$. In this way, the same circular structure of the method of geodesic levelsets is maintained unless a domain boundary or local error intervenes.

Although the same curvature-guided step management could be maintained in this approach, it is not necessarily desirable. While seemingly useful in terms of adapting mesh density to the local neighborhood of the target manifold, this approach was found to add very little in terms of value to the overall method. Specifically, the curvature-guided step management method was found to exclusively decrease inter-levelset step length. In practice, the manifold generator would continue using the default step length until encountering any rough neighborhood. It would then immediately decrease step length to the minimum allowed amount, never to be increased. This is unsurprising, as a rough neighborhood intersecting anywhere with the current levelset would prohibit us from decreasing the inter-levelset step length. As geodesic distance --- and consequently topological circle $C_i$ arc length --- is increased, the likelihood of encountering at least one such neighborhood becomes very large. Given that point placement accuracy is disconnected from inter-levelset step length, the value of this step guidance method is limited to managing the interpolation error associated with mesh triangulation. While this may be critical for some applications, it seems reasonable that an appropriately chosen constant inter-levelset step length may be sufficient.

Departing from the choices of \cite{GeodesicLevelSets}, we decided to refrain from recomputing the half-plane defining tangent vector $\vec{T}$ for every added point. That is, where \cite{GeodesicLevelSets} computed $\vec{T}$ as the vector spanning between the ancestor point nearest neighbors, we elected to compute $\vec{T}$ in the initial levelset, passing it along the corresponding mesh point strand. This modification was prompted as the recomputed tangent vectors of \cite{GeodesicLevelSets} were found to cause mesh structure inconsistencies. Specifically, mesh point strands were found to occasionally intersect. Moreover, this approach to defining $\vec{T}$ causes it to be highly sensitive to the small-scale behavior of $C$ near the considered ancestor point. In some cases, the tangent vector of \cite{GeodesicLevelSets} was found to be poor in terms of representing the larger scale structure of the topological circle. Resulting in irregular nearest neighbor separations and even strand intersections, this behavior prompted us to redefine grid point placement. That is, points are placed along a family of planes defined by constant angles $\{\theta\}$ in the cylinder coordinate system centered on the manifold initial position $\vec{r}_0$ with, the $z$-axis along $\bm{\xi}_3(\vec{r}_0)$. Although not explored in this investigation, there are multiple alternatives to the method of geodesic levelsets that could be investigated with regard to LCS theory application. Several such approaches are summarized by \cite{Survey}.

%[[Could organize into strands instead of  circles -> would make curvature guided step management difficult + insertion -> More resilient to failure, better border management, arguments for choice of method,  inserting ghost points by integration -> could start in the middle and work outward]]

%%=========================================
%\subsection{Alternative methods}


%[[other possible methods for determining manifolds, higher order interpolation between mesh points,]]


%%=========================================
\section{Determining repelling hyperbolic LCSs from manifolds in the Cauchy-Green eigenvector field}\label{sec:discussion_selection}

%[Sensitivity to lABD and Amin + problems with choosing too large, sensitivity to tracer density different approaches with pros and cons, puzzle piece approach vs carveout approach]

Unlike our approach to manifold identification, the construction of repelling hyperbolic LCSs from candidate manifolds was found to be heavily dependent on parameter choice. While the point comparison distance $\epsilon$ of condition D may be chosen based on tracer grid point separation, choosing ABD subdomain tolerance $\Gamma_{\text{ABD}}$ and minimum surface area $A_{\text{min}}$ is largely left up to personal judgment. This leaves room for the user to define these parameters to tailor the result with regard to the number and sizes of LCSs. Although possibly useful for application, this is unsatisfactory from a scientific point of view.

User choice of $\Gamma_{\text{ABD}}$ and $A_{\text{min}}$ should ideally fall into one of two approaches. The first of these, referred to as the cluster approach, uses small values for both $\Gamma_{\text{ABD}}$ and $A_{\text{min}}$ with the intent of treating clusters of overlapping LCS surface elements as single LCSs. While requiring the user to manually, or otherwise, sort these LCS surface elements by structure association, this approach is convincing in terms of low reliance on our tolerance parameters.

As opposed to the cluster approach, what may be termed the carve-out approach utilizes large values for  $\Gamma_{\text{ABD}}$ and $A_{\text{min}}$ to extract LCSs from single manifolds. Specifically, we attempt to choose $\Gamma_{\text{ABD}}$ and $A_{\text{min}}$ such that we get a small number of large LCS surfaces. This approach yields smoother, more coherent LCSs by relying heavily on our tolerance parameters. The result is often a more visually appealing set of LCSs, albeit less convincing in terms of accuracy.

The results presented in chapter \ref{ch:Results} were all computed using the cluster approach, most identified LCS structures consisting of several surface segments. This is particularly evident for the ABC flow data where each identified LCS structure consists of anywhere between $3$ and $23$ LCS surface segments. This approach was found preferable, not only due to its presumed superior accuracy, but also because the carve-out approach was found impractical. Most prominent for the ABC flow test cases, this was because most computed manifolds were too small to extract sufficiently large LCSs. That is, our manifold identification method was often unable to produce sufficiently large manifolds with the allotted time. This issue could be alleviated by implementing some of the aforementioned method alterations, or simply allocating more computation time. Based on practical considerations, manifold construction was limited to $1$ hour of wall clock time, while allowing the process an extra hour to terminate. This margin of safety was introduced as no manifold would be saved in the case of failure to terminate within the reserved cluster wall time. This strategy should ideally be combined with an accompanying reduction of Runge-Kutta method error tolerance parameters, reducing error accumulation as to decrease the prevalence of procedure termination prompted by continuous self-intersections.

%%=========================================
\subsection{Implementing criterion of requiring LCSs to be locally most repelling}\label{sec:ambiguity}

Although simple in mathematical terms, the LCS existence conditions suggested by \cite{Haller12} are not entirely clear-cut in terms of implementation. This specifically pertains to condition 4 in \eqref{eq:ExistenceConditions}, intended to establish the LCS as a local repulsion maximum. It could be argued that simply noting the presence of a local repulsion maximum is of limited value in terms of application. That is, by following \cite{Haller14}'s criteria, we might be selecting large numbers of inconsequential LCSs, cluttering a system interpretation intended to highlight only major system-defining structures. This despite requiring a minimum LCS surface area. We could view this as a weakness of the present approach, it being a rather close adaptation of \cite{Haller12}'s criteria. This shortcoming could possibly be alleviated by either requiring a certain LCS repulsion rate, or by only selecting a single LCS from any given LCS grouping. Note that the latter of these requirements may not be paired with the previously mentioned cluster approach to choosing $\Gamma_{\text{ABD}}$ and $A_{\text{min}}$.

%[[Is the literal interpretation useful? How otherwise to define it?]]

%%=========================================
\subsection{Alternative methods}\label{sec:selection_alternative_methods}

There are several alternative approaches to LCS selection from manifold candidates, each with accompanying advantages and disadvantages, as compared to the present method. A very simple approach could be devised by defining LCS neighborhoods as rectangular cuboids within the domain of interest  $U$. LCS selection could then be carried out by selecting the most repelling of the LCS candidates present in a single neighborhood. Note that as condition $4$ in \eqref{eq:ExistenceConditions} is approximated by this act of comparison and selection, we substitute the ABD subdomain described in section \ref{sec:AB_subdomain} with an analogous AB subdomain. That is, we drop condition D (see equation \eqref{eq:LCS_condition_D}), performing mesh point selection with criteria A and B (see equations \eqref{eq:LCS_condition_A} and \eqref{eq:LCS_condition_B}). While simple in terms of implementation and cheap in terms of computational resources, this method is quite far removed from the theoretical foundation given by condition $4$ in equation \eqref{eq:ExistenceConditions}. This method does however provide a large amount of flexibility in terms of neighborhood selection, requiring the user to exercise personal judgment.

A second possible alternative can be construed by generalizing the approach of \cite{Haller12} to three dimensions. This approach entails computing LCS candidate surfaces using the aforementioned AB subdomain. Intersections of the resulting LCS candidates with a selection of horizontal and vertical planes are then identified and used to define locally neighboring LCS candidates. The most repelling LCS candidate surface within each such plane intersection neighborhood is then selected as an LCS. This approach has the advantage of inherently choosing the most repelling among nearby LCS candidates. It should however be noted that two LCS candidates, identified as neighbors at a plane intersection, may rapidly diverge elsewhere in the domain. This is indeed also a weakness of the method as originally proposed in the two-dimensional case.

Note that while approximating condition $4$ in equation \eqref{eq:ExistenceConditions} more closely than the method of neighborhood volumes, the likely divergence of identified neighbor surfaces may cause major discrepancies from the theoretical definition. More robust algorithms for identifying neighboring LCS candidate surfaces could be conceived by use of various clustering algorithms. 

%%=========================================
\section{Results and prospects for application}\label{sec:discussion_results}

Having applied our method for identification of repelling hyperbolic LCSs to various distinct cases, we are now able to draw some conclusions regarding these systems, as well as the prospects for further use of the method itself. Specifically, the efficacy --- or lack thereof --- of our method in terms of unveiling useful system insights could guide us with regard to further development and utilization.

%%=========================================
\subsection{ABC flow system}\label{sec:discussion_ABC-flow}

As previously discussed in section \ref{sec:discussion_selection}, manifold selection in the ABC flow systems was done according to the cluster approach. In addition to allowing limited use of numerical tolerance, this approach is less dependent on large input manifolds. This latter advantage was found to be crucial in the case of the ABC flow system. That is, manifold development was frequently stunted by complicated eigenvector field regions. Specifically, for the steady and unsteady ABC flow cases, manifolds in the Cauchy-Green eigenvector fields were found to frequently display self-intersections, causing the algorithm to terminate. This behavior was likely caused by small numerical errors making one or more mesh points jump onto a neighboring manifold. In these chaotic regions, even closely neighboring manifolds would diverge significantly from the target manifold, possibly causing the unphysical self-intersections. These issues could possibly be mitigated, either by increasing the accuracy requirements of the selected adaptive step Runge-Kutta method, or eliminating our use of topological circle curve interpolations for inserting intervening mesh points (see section \ref{sec:discussion_GLS}).

The small differences observed between the results of the steady and unsteady ABC flow systems are also interesting. This indicates that although considerable, the added perturbation in the unsteady flow field did not significantly alter the principal system characteristics. Although certainly requiring a more rigorous investigative approach, this result is suggestive of the robustness thought to be characteristic to LCSs. That is, the small differences observed in terms of ABD subdomains and resulting repelling hyperbolic LCSs caused by rather large changes to the underlying flow system, are indicative of LCSs in fact being largely independent of the idiosyncrasies of small scale flow patterns.

Forming large and largely coherent particle transport boundaries, the LCSs detected in the steady and unsteady ABC flow systems exhibit many desirable attributes with regard to possible application. Specifically, the formation of clear boundaries, effectively separating the investigated domain into sections, is exactly the kind of useful insight sought by application of LCS theory. It should however be noted that the observed breakdown of the LCS surface is problematic in terms of some applications. While computing initial position LCSs is sufficient in terms of investigating large scale tracer mixing patterns, we gain little insight with regard to the actual transport barriers. That is, unless we are able to accurately transport our LCS surfaces in time, we are unable to track the transport barriers deemed so desirable in terms of application. Techniques for increasing the accuracy of LCS transport in the two-dimensional case are described by \cite{Haller12}. 

%%=========================================
\subsection{Førde fjord system}\label{sec:discussion_fjord}

Less difficult in terms of manifold development, the Førde fjord data set was successfully analyzed with comparatively small ABD subdomain tolerance $\Gamma_{\text{ABD}}$. The resulting, largely horizontal, LCSs are interesting in that they suggest minimal vertical mixing across horizontal water layers. If possible to reproduce for larger parts of the fjord area, this is a useful insight with regard to the present issue of using the Førde fjord as a deposit for mine tailings.

It could be speculated that this behavior is due to sudden steps in terms of water layer density caused by sharp changes in either salinity or temperature. The exact causes for the observed LCSs is however beyond the scope of this investigation. 

%%=========================================
\subsection{Prospects for application}\label{sec:discussion_application}

Based on our test cases, it seems plausible that the present method --- either after minor modifications, or in its current form --- could be useful in terms of application to real world flow systems. This view is substantiated both from the seemingly valuable insights gained from application to the Førde fjord data set, as well as the seeming robustness of our LCSs to flow field perturbations. The latter argument is particularly interesting as it suggests imperfect gridded data models could be sufficient in order to accurately reproduce real world LCSs.

The most significant detriment to further application of our method of three-dimensional LCS identification is likely its complexity. As computing LCSs in three dimensions is significantly more costly --- both with respect to development and resources --- than its two-dimensional counterpart, strong arguments in terms of result accuracy and insights are needed to justify a change in practices. It is intuitively clear that the argument for using three-dimensional LCS identification is stronger for systems where all three dimensions are of similar importance to overall system characteristics. Fjord flow systems, such as the one exhibited in section \ref{sec:fjord}, may be seen as examples of such systems, their depth often being of similar magnitude to their width and their vertical transport significantly altering system dynamics. Conversely, the surface layer of the oceanic within which buoyant materials, such as some types of plastic, are often contained, is an example of a nearly two-dimensional system.

Supplementing considerations of system dimensions, comparative studies of two- and three-dimensional LCS identification could be used to guide approach selection. This could be done numerically, or in the field by deploying tracers in well-knowns flow systems. Such studies could be very useful in terms of determining the applicability of three-dimensional LCS identification as a part of a larger toolbox consisting of existing and coming methods.

%[seems plausible in some scenarios, IRL tests could be useful, gridded data models accurate enough to gain useful insights?, how much insight is gained by adding third dimension, ease of application to new cases,  (link to notebook) and how to apply to new cases, opposite perspective -> how to maximize mixing, ]

%%=========================================
\section{Recommendations for further work}

As described by \cite{Oettinger}, identification of LCSs in three-dimensional systems has traditionally been done by identifying two-dimensional LCS layers, combining them to surfaces by use of interpolation. This approach, outlined by \cite{Blazevski}, is clearly problematic as it completely ignores all $3$\ts{rd}-dimension system dynamics. Moreover, the method proposed by \cite{Oettinger}, while flexible, does not yield coherent surfaces conducive to further analysis. However, as suggested by \cite{Oettinger}, combining LCS theory with dedicated manifold identification methods yields a more practical approach to identification of three-dimensional hyperbolic LCSs.

Further inquiries as to LCS identification in three-dimensional systems could focus on different methods for reconstructing manifolds. Several such dedicated methods, including the method of geodesic levelsets utilized in this investigation, are outlined by \cite{Survey}. An investigation of the efficacy of these various methods with regard to LCS identification in three-dimensional systems could be of great value with respect to application. This also pertains to the various LCS extraction approaches outlined in section \ref{sec:discussion_selection}.

More specific to the current approach, exploring the previously described method alterations could potentially yield improvements in terms of comprehensibility, speed, and accuracy. Particularly, departing from the strict ordering of mesh points into levelsets could allow better boundary treatment, as well as general improvements in terms of accuracy (see section \ref{sec:discussion_GLS}). Sensitivity analysis with regard to ODE solver error tolerances, as well as interpolation order and tracer grid density, could also prove valuable in terms of identifying sources of error.

Furthermore, as many flow systems may reasonably be approximated as two-dimensional, comparative studies of two-dimensional and three-dimensional approaches to LCS identification could be very useful as to determine proper application of each tool. That is, for what applications are the comparatively simpler two-dimensional approaches sufficient to capture the most critical system dynamics.

Finally, combining available gridded data models with physical tracers deployed in real world flow systems could allow us to verify the robustness of our LCS identification tools to the inaccuracies of these data models. One such pilot experiment, known as the \textit{NSF-ALPHA Sea Experiment 2017} was conducted by \cite{PeacockTracers}. A similar approach was applied to the Scott Reef atoll in Western Australia by \cite{Filippi}. Further experiments are planned by the \textit{NSF-ALPHA} project in the near future.

%[IRL tracer trials, implement optimizations and different ordering, compare with 2D reference method, try different manifold and selection mehtods]